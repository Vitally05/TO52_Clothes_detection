{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "977b9487-f763-4ec4-a2e2-5f9c3041371d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T14:12:07.427375Z",
     "start_time": "2024-05-03T14:12:07.424384Z"
    }
   },
   "outputs": [],
   "source": [
    "import ultralytics\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "from IPython.display import display, Image\n",
    "from IPython import display\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[2K\n",
      "\u001B[2K\n",
      "Ultralytics YOLOv8.1.27 Ã°Å¸Å¡â‚¬ Python-3.12.2 torch-2.3.0+cu118 CUDA:0 (NVIDIA GeForce RTX 4070 Laptop GPU, 8188MiB)\n",
      "Setup complete Ã¢Å“â€¦ (32 CPUs, 31.7 GB RAM, 243.7/928.4 GB disk)\n",
      "\n",
      "OS                  Windows-11-10.0.22631-SP0\n",
      "Environment         Windows\n",
      "Python              3.12.2\n",
      "Install             pip\n",
      "RAM                 31.69 GB\n",
      "CPU                 13th Gen Intel Core(TM) i9-13900HX\n",
      "CUDA                11.8\n",
      "\n",
      "matplotlib          Ã¢Å“â€¦ 3.8.3>=3.3.0\n",
      "opencv-python       Ã¢Å“â€¦ 4.9.0.80>=4.6.0\n",
      "pillow              Ã¢Å“â€¦ 10.2.0>=7.1.2\n",
      "pyyaml              Ã¢Å“â€¦ 6.0.1>=5.3.1\n",
      "requests            Ã¢Å“â€¦ 2.31.0>=2.23.0\n",
      "scipy               Ã¢Å“â€¦ 1.12.0>=1.4.1\n",
      "torch               Ã¢Å“â€¦ 2.3.0+cu118>=1.8.0\n",
      "torchvision         Ã¢Å“â€¦ 0.18.0+cu118>=0.9.0\n",
      "tqdm                Ã¢Å“â€¦ 4.66.2>=4.64.0\n",
      "psutil              Ã¢Å“â€¦ 5.9.8\n",
      "py-cpuinfo          Ã¢Å“â€¦ 9.0.0\n",
      "thop                Ã¢Å“â€¦ 0.1.1-2209072238>=0.1.1\n",
      "pandas              Ã¢Å“â€¦ 2.2.1>=1.1.4\n",
      "seaborn             Ã¢Å“â€¦ 0.13.2>=0.11.0\n"
     ]
    }
   ],
   "source": [
    "display.clear_output()\n",
    "!yolo checks"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1b58065ae9eeb92",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.27 ðŸš€ Python-3.12.2 torch-2.2.1+cpu CPU (13th Gen Intel Core(TM) i9-13900HX)\n",
      "Setup complete âœ… (32 CPUs, 31.7 GB RAM, 208.6/928.4 GB disk)\n"
     ]
    }
   ],
   "source": [
    "ultralytics.checks()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "46335d486b444567",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri May  3 09:09:17 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 551.78                 Driver Version: 551.78         CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4070 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   46C    P4             13W /   45W |       0MiB /   8188MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T07:09:18.102824Z",
     "start_time": "2024-05-03T07:09:15.828216Z"
    }
   },
   "id": "a61543751245783e",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T14:12:31.153314Z",
     "start_time": "2024-05-03T14:12:31.148353Z"
    }
   },
   "id": "38fc73e94f1e653d",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0.], device='cuda:0')"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(1).cuda()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T14:12:44.870960Z",
     "start_time": "2024-05-03T14:12:44.542793Z"
    }
   },
   "id": "4442081534188748",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.3.0+cu118\n",
      "CUDA Available: True\n",
      "CUDA Version: 11.8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Version:\", torch.version.cuda)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T14:12:47.441889Z",
     "start_time": "2024-05-03T14:12:47.438691Z"
    }
   },
   "id": "655fc9cbab34759c",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": "YOLO(\n  (model): DetectionModel(\n    (model): Sequential(\n      (0): Conv(\n        (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (1): Conv(\n        (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (2): C2f(\n        (cv1): Conv(\n          (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): ModuleList(\n          (0): Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n          )\n        )\n      )\n      (3): Conv(\n        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (4): C2f(\n        (cv1): Conv(\n          (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): ModuleList(\n          (0-1): 2 x Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n          )\n        )\n      )\n      (5): Conv(\n        (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (6): C2f(\n        (cv1): Conv(\n          (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): ModuleList(\n          (0-1): 2 x Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n          )\n        )\n      )\n      (7): Conv(\n        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (8): C2f(\n        (cv1): Conv(\n          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): ModuleList(\n          (0): Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n          )\n        )\n      )\n      (9): SPPF(\n        (cv1): Conv(\n          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n      )\n      (10): Upsample(scale_factor=2.0, mode='nearest')\n      (11): Concat()\n      (12): C2f(\n        (cv1): Conv(\n          (conv): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): ModuleList(\n          (0): Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n          )\n        )\n      )\n      (13): Upsample(scale_factor=2.0, mode='nearest')\n      (14): Concat()\n      (15): C2f(\n        (cv1): Conv(\n          (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): ModuleList(\n          (0): Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n          )\n        )\n      )\n      (16): Conv(\n        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (17): Concat()\n      (18): C2f(\n        (cv1): Conv(\n          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): ModuleList(\n          (0): Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n          )\n        )\n      )\n      (19): Conv(\n        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (20): Concat()\n      (21): C2f(\n        (cv1): Conv(\n          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): ModuleList(\n          (0): Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n          )\n        )\n      )\n      (22): Detect(\n        (cv2): ModuleList(\n          (0): Sequential(\n            (0): Conv(\n              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (1): Conv(\n              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n          )\n          (1): Sequential(\n            (0): Conv(\n              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (1): Conv(\n              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n          )\n          (2): Sequential(\n            (0): Conv(\n              (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (1): Conv(\n              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n          )\n        )\n        (cv3): ModuleList(\n          (0): Sequential(\n            (0): Conv(\n              (conv): Conv2d(64, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (1): Conv(\n              (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (2): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n          )\n          (1): Sequential(\n            (0): Conv(\n              (conv): Conv2d(128, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (1): Conv(\n              (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (2): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n          )\n          (2): Sequential(\n            (0): Conv(\n              (conv): Conv2d(256, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (1): Conv(\n              (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (2): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n          )\n        )\n        (dfl): DFL(\n          (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        )\n      )\n    )\n  )\n)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device: str = \"nvidia\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "# Load a model\n",
    "model = YOLO(\"yolov8n.yaml\")  # build a new model from scratch\n",
    "model.to(device)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T11:49:43.406340Z",
     "start_time": "2024-05-03T11:49:43.259743Z"
    }
   },
   "id": "f0ada3815fd71cc8",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b93f7585-6bbe-4632-bf81-513244bd8c0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T14:12:48.322105Z",
     "start_time": "2024-04-19T14:05:35.372896Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.2.2 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.1.27 ðŸš€ Python-3.12.2 torch-2.2.1+cpu CPU (13th Gen Intel Core(TM) i9-13900HX)\n",
      "\u001B[34m\u001B[1mengine\\trainer: \u001B[0mtask=detect, mode=train, model=yolov8n.yaml, data=config.yaml, epochs=1, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train12, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train12\n",
      "Overriding model.yaml nc=80 with nc=562\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1   1226590  ultralytics.nn.modules.head.Detect           [562, [64, 128, 256]]         \n",
      "YOLOv8n summary: 225 layers, 3486126 parameters, 3486110 gradients, 10.5 GFLOPs\n",
      "\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mtrain: \u001B[0mScanning C:\\Users\\elise\\Documents\\1 UTBM\\INFO 4 - DS\\TO52\\data\\img\\1981_Graphic_Ringer_Tee.cache... 0 images, 289219 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 289219/289219 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING âš ï¸ No labels found in C:\\Users\\elise\\Documents\\1 UTBM\\INFO 4 - DS\\TO52\\data\\img\\1981_Graphic_Ringer_Tee.cache, training may not work correctly. See https://docs.ultralytics.com/datasets/detect for dataset formatting guidance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mval: \u001B[0mScanning C:\\Users\\elise\\Documents\\1 UTBM\\INFO 4 - DS\\TO52\\data\\img\\1981_Graphic_Ringer_Tee.cache... 0 images, 289219 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 289219/289219 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING âš ï¸ No labels found in C:\\Users\\elise\\Documents\\1 UTBM\\INFO 4 - DS\\TO52\\data\\img\\1981_Graphic_Ringer_Tee.cache, training may not work correctly. See https://docs.ultralytics.com/datasets/detect for dataset formatting guidance.\n",
      "Plotting labels to runs\\detect\\train12\\labels.jpg... \n",
      "zero-size array to reduction operation maximum which has no identity\n",
      "\u001B[34m\u001B[1moptimizer:\u001B[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001B[34m\u001B[1moptimizer:\u001B[0m AdamW(lr=1.8e-05, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001B[1mruns\\detect\\train12\u001B[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/1         0G          0      145.6          0          0        640:   1%|          | 126/18077 [06:33<15:34:29,  3.12s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m model \u001B[38;5;241m=\u001B[39m YOLO(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124myolov8n.yaml\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 2\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mconfig.yaml\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\engine\\model.py:655\u001B[0m, in \u001B[0;36mModel.train\u001B[1;34m(self, trainer, **kwargs)\u001B[0m\n\u001B[0;32m    652\u001B[0m             \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[0;32m    654\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mhub_session \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msession  \u001B[38;5;66;03m# attach optional HUB session\u001B[39;00m\n\u001B[1;32m--> 655\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    656\u001B[0m \u001B[38;5;66;03m# Update model and cfg after training\u001B[39;00m\n\u001B[0;32m    657\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m RANK \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m0\u001B[39m):\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:213\u001B[0m, in \u001B[0;36mBaseTrainer.train\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    210\u001B[0m         ddp_cleanup(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28mstr\u001B[39m(file))\n\u001B[0;32m    212\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 213\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_train\u001B[49m\u001B[43m(\u001B[49m\u001B[43mworld_size\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:381\u001B[0m, in \u001B[0;36mBaseTrainer._do_train\u001B[1;34m(self, world_size)\u001B[0m\n\u001B[0;32m    379\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mamp\u001B[38;5;241m.\u001B[39mautocast(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mamp):\n\u001B[0;32m    380\u001B[0m     batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpreprocess_batch(batch)\n\u001B[1;32m--> 381\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss_items \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    382\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m RANK \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m    383\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m=\u001B[39m world_size\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:88\u001B[0m, in \u001B[0;36mBaseModel.forward\u001B[1;34m(self, x, *args, **kwargs)\u001B[0m\n\u001B[0;32m     78\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     79\u001B[0m \u001B[38;5;124;03mForward pass of the model on a single scale. Wrapper for `_forward_once` method.\u001B[39;00m\n\u001B[0;32m     80\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     85\u001B[0m \u001B[38;5;124;03m    (torch.Tensor): The output of the network.\u001B[39;00m\n\u001B[0;32m     86\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     87\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(x, \u001B[38;5;28mdict\u001B[39m):  \u001B[38;5;66;03m# for cases of training and validating while training.\u001B[39;00m\n\u001B[1;32m---> 88\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     89\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredict(x, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:267\u001B[0m, in \u001B[0;36mBaseModel.loss\u001B[1;34m(self, batch, preds)\u001B[0m\n\u001B[0;32m    264\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcriterion \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minit_criterion()\n\u001B[0;32m    266\u001B[0m preds \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mforward(batch[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mimg\u001B[39m\u001B[38;5;124m\"\u001B[39m]) \u001B[38;5;28;01mif\u001B[39;00m preds \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m preds\n\u001B[1;32m--> 267\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcriterion\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpreds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\utils\\loss.py:234\u001B[0m, in \u001B[0;36mv8DetectionLoss.__call__\u001B[1;34m(self, preds, batch)\u001B[0m\n\u001B[0;32m    230\u001B[0m target_scores_sum \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmax\u001B[39m(target_scores\u001B[38;5;241m.\u001B[39msum(), \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m    232\u001B[0m \u001B[38;5;66;03m# Cls loss\u001B[39;00m\n\u001B[0;32m    233\u001B[0m \u001B[38;5;66;03m# loss[1] = self.varifocal_loss(pred_scores, target_scores, target_labels) / target_scores_sum  # VFL way\u001B[39;00m\n\u001B[1;32m--> 234\u001B[0m loss[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbce\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpred_scores\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget_scores\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39msum() \u001B[38;5;241m/\u001B[39m target_scores_sum  \u001B[38;5;66;03m# BCE\u001B[39;00m\n\u001B[0;32m    236\u001B[0m \u001B[38;5;66;03m# Bbox loss\u001B[39;00m\n\u001B[0;32m    237\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m fg_mask\u001B[38;5;241m.\u001B[39msum():\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:725\u001B[0m, in \u001B[0;36mBCEWithLogitsLoss.forward\u001B[1;34m(self, input, target)\u001B[0m\n\u001B[0;32m    724\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor, target: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 725\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbinary_cross_entropy_with_logits\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    726\u001B[0m \u001B[43m                                              \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    727\u001B[0m \u001B[43m                                              \u001B[49m\u001B[43mpos_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpos_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    728\u001B[0m \u001B[43m                                              \u001B[49m\u001B[43mreduction\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreduction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\functional.py:3199\u001B[0m, in \u001B[0;36mbinary_cross_entropy_with_logits\u001B[1;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001B[0m\n\u001B[0;32m   3196\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (target\u001B[38;5;241m.\u001B[39msize() \u001B[38;5;241m==\u001B[39m \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39msize()):\n\u001B[0;32m   3197\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTarget size (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtarget\u001B[38;5;241m.\u001B[39msize()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m) must be the same as input size (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39msize()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m-> 3199\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbinary_cross_entropy_with_logits\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpos_weight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreduction_enum\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "model = YOLO('yolov8n.yaml')\n",
    "model.train(data='config.yaml', epochs=1)\n",
    "#Tous blouse et cardigan --> trop long"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.2.2 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.1.27 ðŸš€ Python-3.12.2 torch-2.2.1+cpu CPU (13th Gen Intel Core(TM) i9-13900HX)\n",
      "\u001B[34m\u001B[1mengine\\trainer: \u001B[0mtask=detect, mode=train, model=yolov8n.yaml, data=config.yaml, epochs=1, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train\n",
      "Overriding model.yaml nc=80 with nc=562\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1   1226590  ultralytics.nn.modules.head.Detect           [562, [64, 128, 256]]         \n",
      "YOLOv8n summary: 225 layers, 3486126 parameters, 3486110 gradients, 10.5 GFLOPs\n",
      "\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mtrain: \u001B[0mScanning C:\\Users\\elise\\Documents\\1 UTBM\\INFO 4 - DS\\TO52\\otherdata\\img\\Striped_Mandarin_Collar_Blouse... 0 images, 3287 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3287/3287 [00:00<00:00, 8353.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mtrain: \u001B[0mWARNING âš ï¸ No labels found in C:\\Users\\elise\\Documents\\1 UTBM\\INFO 4 - DS\\TO52\\otherdata\\img\\Striped_Mandarin_Collar_Blouse.cache. See https://docs.ultralytics.com/datasets/detect for dataset formatting guidance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mtrain: \u001B[0mNew cache created: C:\\Users\\elise\\Documents\\1 UTBM\\INFO 4 - DS\\TO52\\otherdata\\img\\Striped_Mandarin_Collar_Blouse.cache\n",
      "WARNING âš ï¸ No labels found in C:\\Users\\elise\\Documents\\1 UTBM\\INFO 4 - DS\\TO52\\otherdata\\img\\Striped_Mandarin_Collar_Blouse.cache, training may not work correctly. See https://docs.ultralytics.com/datasets/detect for dataset formatting guidance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mval: \u001B[0mScanning C:\\Users\\elise\\Documents\\1 UTBM\\INFO 4 - DS\\TO52\\otherdata\\img\\Striped_Mandarin_Collar_Blouse.cache... 0 images, 3287 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3287/3287 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING âš ï¸ No labels found in C:\\Users\\elise\\Documents\\1 UTBM\\INFO 4 - DS\\TO52\\otherdata\\img\\Striped_Mandarin_Collar_Blouse.cache, training may not work correctly. See https://docs.ultralytics.com/datasets/detect for dataset formatting guidance.\n",
      "Plotting labels to runs\\detect\\train\\labels.jpg... \n",
      "zero-size array to reduction operation maximum which has no identity\n",
      "\u001B[34m\u001B[1moptimizer:\u001B[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001B[34m\u001B[1moptimizer:\u001B[0m AdamW(lr=1.8e-05, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001B[1mruns\\detect\\train\u001B[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "        1/1         0G          0      131.1          0          0        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 206/206 [10:34<00:00,  3.08s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   1%|          | 1/103 [00:01<02:50,  1.67s/it]Exception in thread Thread-108 (plot_images):\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\elise\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1073, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\elise\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 761, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"C:\\Users\\elise\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1010, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\elise\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\utils\\plotting.py\", line 781, in plot_images\n",
      "    c = names.get(c, c) if names else c\n",
      "        ^^^^^^^^^\n",
      "AttributeError: 'str' object has no attribute 'get'\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103/103 [04:28<00:00,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3287          0          0          0          0          0\n",
      "WARNING âš ï¸ no labels found in detect set, can not compute metrics without labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m model \u001B[38;5;241m=\u001B[39m YOLO(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124myolov8n.yaml\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 2\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mconfig.yaml\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\engine\\model.py:655\u001B[0m, in \u001B[0;36mModel.train\u001B[1;34m(self, trainer, **kwargs)\u001B[0m\n\u001B[0;32m    652\u001B[0m             \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[0;32m    654\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mhub_session \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msession  \u001B[38;5;66;03m# attach optional HUB session\u001B[39;00m\n\u001B[1;32m--> 655\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    656\u001B[0m \u001B[38;5;66;03m# Update model and cfg after training\u001B[39;00m\n\u001B[0;32m    657\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m RANK \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m0\u001B[39m):\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:213\u001B[0m, in \u001B[0;36mBaseTrainer.train\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    210\u001B[0m         ddp_cleanup(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28mstr\u001B[39m(file))\n\u001B[0;32m    212\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 213\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_train\u001B[49m\u001B[43m(\u001B[49m\u001B[43mworld_size\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:429\u001B[0m, in \u001B[0;36mBaseTrainer._do_train\u001B[1;34m(self, world_size)\u001B[0m\n\u001B[0;32m    427\u001B[0m \u001B[38;5;66;03m# Validation\u001B[39;00m\n\u001B[0;32m    428\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mval \u001B[38;5;129;01mor\u001B[39;00m final_epoch \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstopper\u001B[38;5;241m.\u001B[39mpossible_stop \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstop:\n\u001B[1;32m--> 429\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetrics, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfitness \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalidate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    430\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msave_metrics(metrics\u001B[38;5;241m=\u001B[39m{\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlabel_loss_items(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtloss), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetrics, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlr})\n\u001B[0;32m    431\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstop \u001B[38;5;241m|\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstopper(epoch \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfitness) \u001B[38;5;129;01mor\u001B[39;00m final_epoch\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:551\u001B[0m, in \u001B[0;36mBaseTrainer.validate\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    545\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mvalidate\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    546\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    547\u001B[0m \u001B[38;5;124;03m    Runs validation on test set using self.validator.\u001B[39;00m\n\u001B[0;32m    548\u001B[0m \n\u001B[0;32m    549\u001B[0m \u001B[38;5;124;03m    The returned dict is expected to contain \"fitness\" key.\u001B[39;00m\n\u001B[0;32m    550\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 551\u001B[0m     metrics \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalidator\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    552\u001B[0m     fitness \u001B[38;5;241m=\u001B[39m metrics\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfitness\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy())  \u001B[38;5;66;03m# use loss as fitness measure if not found\u001B[39;00m\n\u001B[0;32m    553\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbest_fitness \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbest_fitness \u001B[38;5;241m<\u001B[39m fitness:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[1;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\engine\\validator.py:199\u001B[0m, in \u001B[0;36mBaseValidator.__call__\u001B[1;34m(self, trainer, model)\u001B[0m\n\u001B[0;32m    197\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspeed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(\u001B[38;5;28mzip\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspeed\u001B[38;5;241m.\u001B[39mkeys(), (x\u001B[38;5;241m.\u001B[39mt \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataloader\u001B[38;5;241m.\u001B[39mdataset) \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m1e3\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m dt)))\n\u001B[0;32m    198\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfinalize_metrics()\n\u001B[1;32m--> 199\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprint_results\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    200\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrun_callbacks(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mon_val_end\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    201\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\models\\yolo\\detect\\val.py:191\u001B[0m, in \u001B[0;36mDetectionValidator.print_results\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mplots:\n\u001B[0;32m    189\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m normalize \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m, \u001B[38;5;28;01mFalse\u001B[39;00m:\n\u001B[0;32m    190\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfusion_matrix\u001B[38;5;241m.\u001B[39mplot(\n\u001B[1;32m--> 191\u001B[0m             save_dir\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msave_dir, names\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnames\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalues\u001B[49m(), normalize\u001B[38;5;241m=\u001B[39mnormalize, on_plot\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_plot\n\u001B[0;32m    192\u001B[0m         )\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'str' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "model = YOLO('yolov8n.yaml')\n",
    "model.train(data='config.yaml', epochs=1)\n",
    "#Qqe blouses : error"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-19T15:01:11.018735Z",
     "start_time": "2024-04-19T14:46:05.343086Z"
    }
   },
   "id": "30d252e23c581a28",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.2.2 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.1.27 ðŸš€ Python-3.12.2 torch-2.2.1+cpu CPU (13th Gen Intel Core(TM) i9-13900HX)\n",
      "\u001B[34m\u001B[1mengine\\trainer: \u001B[0mtask=detect, mode=train, model=yolov8n.yaml, data=config.yaml, epochs=1, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train5, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train5\n",
      "Overriding model.yaml nc=80 with nc=562\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1   1226590  ultralytics.nn.modules.head.Detect           [562, [64, 128, 256]]         \n",
      "YOLOv8n summary: 225 layers, 3486126 parameters, 3486110 gradients, 10.5 GFLOPs\n",
      "\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mtrain: \u001B[0mScanning C:\\Users\\elise\\Documents\\1 UTBM\\INFO 4 - DS\\TO52\\otherdata\\img\\Windowpane_Floral_Crochet_Blouse... 0 images, 423 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 423/423 [00:00<00:00, 7784.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mtrain: \u001B[0mWARNING âš ï¸ No labels found in C:\\Users\\elise\\Documents\\1 UTBM\\INFO 4 - DS\\TO52\\otherdata\\img\\Windowpane_Floral_Crochet_Blouse.cache. See https://docs.ultralytics.com/datasets/detect for dataset formatting guidance.\n",
      "\u001B[34m\u001B[1mtrain: \u001B[0mNew cache created: C:\\Users\\elise\\Documents\\1 UTBM\\INFO 4 - DS\\TO52\\otherdata\\img\\Windowpane_Floral_Crochet_Blouse.cache\n",
      "WARNING âš ï¸ No labels found in C:\\Users\\elise\\Documents\\1 UTBM\\INFO 4 - DS\\TO52\\otherdata\\img\\Windowpane_Floral_Crochet_Blouse.cache, training may not work correctly. See https://docs.ultralytics.com/datasets/detect for dataset formatting guidance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[34m\u001B[1mval: \u001B[0mScanning C:\\Users\\elise\\Documents\\1 UTBM\\INFO 4 - DS\\TO52\\otherdata\\img\\Windowpane_Floral_Crochet_Blouse.cache... 0 images, 423 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 423/423 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING âš ï¸ No labels found in C:\\Users\\elise\\Documents\\1 UTBM\\INFO 4 - DS\\TO52\\otherdata\\img\\Windowpane_Floral_Crochet_Blouse.cache, training may not work correctly. See https://docs.ultralytics.com/datasets/detect for dataset formatting guidance.\n",
      "Plotting labels to runs\\detect\\train5\\labels.jpg... \n",
      "zero-size array to reduction operation maximum which has no identity\n",
      "\u001B[34m\u001B[1moptimizer:\u001B[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001B[34m\u001B[1moptimizer:\u001B[0m AdamW(lr=1.8e-05, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001B[1mruns\\detect\\train5\u001B[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "        1/1         0G          0      144.8          0          0        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [01:07<00:00,  2.50s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:32<00:00,  2.34s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "torch.cat(): expected a non-empty list of Tensors",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[18], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m model \u001B[38;5;241m=\u001B[39m YOLO(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124myolov8n.yaml\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 2\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mconfig.yaml\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\engine\\model.py:655\u001B[0m, in \u001B[0;36mModel.train\u001B[1;34m(self, trainer, **kwargs)\u001B[0m\n\u001B[0;32m    652\u001B[0m             \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[0;32m    654\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mhub_session \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msession  \u001B[38;5;66;03m# attach optional HUB session\u001B[39;00m\n\u001B[1;32m--> 655\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    656\u001B[0m \u001B[38;5;66;03m# Update model and cfg after training\u001B[39;00m\n\u001B[0;32m    657\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m RANK \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m0\u001B[39m):\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:213\u001B[0m, in \u001B[0;36mBaseTrainer.train\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    210\u001B[0m         ddp_cleanup(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28mstr\u001B[39m(file))\n\u001B[0;32m    212\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 213\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_train\u001B[49m\u001B[43m(\u001B[49m\u001B[43mworld_size\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:429\u001B[0m, in \u001B[0;36mBaseTrainer._do_train\u001B[1;34m(self, world_size)\u001B[0m\n\u001B[0;32m    427\u001B[0m \u001B[38;5;66;03m# Validation\u001B[39;00m\n\u001B[0;32m    428\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mval \u001B[38;5;129;01mor\u001B[39;00m final_epoch \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstopper\u001B[38;5;241m.\u001B[39mpossible_stop \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstop:\n\u001B[1;32m--> 429\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetrics, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfitness \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalidate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    430\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msave_metrics(metrics\u001B[38;5;241m=\u001B[39m{\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlabel_loss_items(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtloss), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetrics, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlr})\n\u001B[0;32m    431\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstop \u001B[38;5;241m|\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstopper(epoch \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfitness) \u001B[38;5;129;01mor\u001B[39;00m final_epoch\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:551\u001B[0m, in \u001B[0;36mBaseTrainer.validate\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    545\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mvalidate\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    546\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    547\u001B[0m \u001B[38;5;124;03m    Runs validation on test set using self.validator.\u001B[39;00m\n\u001B[0;32m    548\u001B[0m \n\u001B[0;32m    549\u001B[0m \u001B[38;5;124;03m    The returned dict is expected to contain \"fitness\" key.\u001B[39;00m\n\u001B[0;32m    550\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 551\u001B[0m     metrics \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalidator\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    552\u001B[0m     fitness \u001B[38;5;241m=\u001B[39m metrics\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfitness\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy())  \u001B[38;5;66;03m# use loss as fitness measure if not found\u001B[39;00m\n\u001B[0;32m    553\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbest_fitness \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbest_fitness \u001B[38;5;241m<\u001B[39m fitness:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[1;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\engine\\validator.py:195\u001B[0m, in \u001B[0;36mBaseValidator.__call__\u001B[1;34m(self, trainer, model)\u001B[0m\n\u001B[0;32m    192\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mplot_predictions(batch, preds, batch_i)\n\u001B[0;32m    194\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrun_callbacks(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mon_val_batch_end\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 195\u001B[0m stats \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_stats\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    196\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcheck_stats(stats)\n\u001B[0;32m    197\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspeed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(\u001B[38;5;28mzip\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspeed\u001B[38;5;241m.\u001B[39mkeys(), (x\u001B[38;5;241m.\u001B[39mt \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataloader\u001B[38;5;241m.\u001B[39mdataset) \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m1e3\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m dt)))\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\models\\yolo\\detect\\val.py:168\u001B[0m, in \u001B[0;36mDetectionValidator.get_stats\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    166\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_stats\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    167\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Returns metrics statistics and results dictionary.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 168\u001B[0m     stats \u001B[38;5;241m=\u001B[39m {k: \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy() \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstats\u001B[38;5;241m.\u001B[39mitems()}  \u001B[38;5;66;03m# to numpy\u001B[39;00m\n\u001B[0;32m    169\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(stats) \u001B[38;5;129;01mand\u001B[39;00m stats[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtp\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39many():\n\u001B[0;32m    170\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetrics\u001B[38;5;241m.\u001B[39mprocess(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mstats)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: torch.cat(): expected a non-empty list of Tensors"
     ]
    }
   ],
   "source": [
    "model = YOLO('yolov8n.yaml')\n",
    "model.train(data='config.yaml', epochs=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-19T15:21:09.547282Z",
     "start_time": "2024-04-19T15:19:26.705124Z"
    }
   },
   "id": "73f905a38687be5a",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.27 ðŸš€ Python-3.12.2 torch-2.2.1+cpu CPU (13th Gen Intel Core(TM) i9-13900HX)\n",
      "\u001B[34m\u001B[1mengine\\trainer: \u001B[0mtask=detect, mode=train, model=yolov8n.yaml, data=config.yaml, epochs=1, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train\n",
      "Overriding model.yaml nc=80 with nc=46\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    760282  ultralytics.nn.modules.head.Detect           [46, [64, 128, 256]]          \n",
      "YOLOv8n summary: 225 layers, 3019818 parameters, 3019802 gradients, 8.2 GFLOPs\n",
      "\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mtrain: \u001B[0mScanning C:\\Users\\elise\\Documents\\1 UTBM\\INFO 4 - DS\\TO52\\otherdata\\labels... 24557 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24557/24557 [00:10<00:00, 2337.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mtrain: \u001B[0mNew cache created: C:\\Users\\elise\\Documents\\1 UTBM\\INFO 4 - DS\\TO52\\otherdata\\labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mval: \u001B[0mScanning C:\\Users\\elise\\Documents\\1 UTBM\\INFO 4 - DS\\TO52\\otherdata\\labels.cache... 24557 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24557/24557 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train\\labels.jpg... \n",
      "\u001B[34m\u001B[1moptimizer:\u001B[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001B[34m\u001B[1moptimizer:\u001B[0m AdamW(lr=0.0002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001B[1mruns\\detect\\train\u001B[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/1         0G      2.765      3.905      3.659         37        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1535/1535 [50:50<00:00,  1.99s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 768/768 [15:43<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      24557      24557      0.697      0.677      0.701      0.324\n",
      "\n",
      "1 epochs completed in 1.110 hours.\n",
      "Optimizer stripped from runs\\detect\\train\\weights\\last.pt, 6.2MB\n",
      "Optimizer stripped from runs\\detect\\train\\weights\\best.pt, 6.2MB\n",
      "\n",
      "Validating runs\\detect\\train\\weights\\best.pt...\n",
      "Ultralytics YOLOv8.1.27 ðŸš€ Python-3.12.2 torch-2.2.1+cpu CPU (13th Gen Intel Core(TM) i9-13900HX)\n",
      "YOLOv8n summary (fused): 168 layers, 3014618 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 768/768 [11:48<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      24557      24557      0.696      0.677      0.702      0.324\n",
      "                Blouse      24557      24557      0.696      0.677      0.702      0.324\n",
      "Speed: 0.5ms preprocess, 25.1ms inference, 0.0ms loss, 0.8ms postprocess per image\n",
      "Results saved to \u001B[1mruns\\detect\\train\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "ultralytics.utils.metrics.DetMetrics object with attributes:\n\nap_class_index: array([2])\nbox: ultralytics.utils.metrics.Metric object\nconfusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x000001CB094DC7A0>\ncurves: ['Precision-Recall(B)', 'F1-Confidence(B)', 'Precision-Confidence(B)', 'Recall-Confidence(B)']\ncurves_results: [[array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[          1,     0.95745,     0.95745,     0.95745,     0.94444,     0.94086,     0.94086,     0.94086,     0.93902,     0.93902,     0.93668,     0.93668,     0.93668,     0.93668,     0.93668,      0.9364,      0.9364,      0.9364,      0.9318,      0.9318,      0.9318,      0.9318,      0.9318,\n             0.9318,      0.9318,      0.9318,      0.9318,      0.9318,      0.9318,      0.9318,      0.9318,      0.9318,      0.9318,      0.9318,      0.9318,      0.9318,      0.9318,      0.9318,      0.9318,      0.9318,      0.9318,      0.9318,      0.9318,      0.9318,      0.9318,      0.9318,\n             0.9318,      0.9318,      0.9318,      0.9318,      0.9318,      0.9318,      0.9318,      0.9318,      0.9318,      0.9318,      0.9318,      0.9318,      0.9318,      0.9318,      0.9318,      0.9318,     0.93061,     0.93002,     0.92943,     0.92943,     0.92943,     0.92943,     0.92943,\n            0.92943,     0.92943,     0.92943,     0.92943,     0.92832,     0.92832,     0.92764,     0.92752,     0.92752,      0.9263,     0.92558,     0.92513,     0.92427,     0.92427,     0.92412,     0.92311,     0.92291,     0.92291,     0.92179,     0.92172,     0.92172,     0.92073,     0.92073,\n            0.92073,     0.92073,     0.92073,     0.92073,     0.92073,     0.92073,     0.92073,     0.92073,     0.92073,     0.92073,     0.92073,     0.92073,     0.92073,     0.92073,     0.92073,     0.92073,     0.92073,     0.92073,     0.92073,     0.92073,     0.92073,     0.92073,     0.92073,\n            0.92073,     0.92073,     0.92073,     0.92073,     0.92073,     0.92073,     0.92073,     0.92073,     0.92073,     0.92073,     0.92073,     0.92073,     0.92073,     0.92073,     0.92073,     0.92068,     0.92067,      0.9206,     0.92006,     0.92006,     0.92006,     0.92006,     0.92006,\n            0.92006,     0.91991,     0.91911,      0.9188,     0.91872,     0.91834,     0.91811,     0.91757,     0.91749,     0.91749,     0.91725,     0.91715,     0.91656,     0.91628,     0.91628,     0.91628,      0.9159,     0.91567,     0.91565,     0.91537,     0.91521,     0.91476,     0.91476,\n            0.91476,     0.91476,     0.91467,     0.91448,     0.91448,     0.91448,     0.91448,     0.91414,     0.91348,     0.91348,     0.91348,     0.91317,     0.91253,     0.91211,     0.91211,     0.91182,      0.9114,     0.91121,     0.91118,     0.91118,     0.91118,     0.91118,     0.91118,\n            0.91118,     0.91118,     0.91117,     0.91104,     0.91073,     0.91073,      0.9102,      0.9102,     0.91018,     0.90972,     0.90951,     0.90923,     0.90923,     0.90853,     0.90852,      0.9085,     0.90829,     0.90829,     0.90829,     0.90829,     0.90829,     0.90829,     0.90829,\n            0.90829,     0.90829,     0.90829,     0.90829,     0.90814,     0.90789,     0.90736,      0.9066,      0.9066,     0.90648,     0.90507,     0.90493,      0.9043,     0.90389,      0.9031,     0.90307,     0.90299,     0.90293,     0.90276,     0.90276,     0.90265,     0.90265,     0.90187,\n            0.90185,     0.90185,     0.90153,     0.90097,     0.90067,      0.9003,     0.90005,     0.89988,     0.89988,     0.89943,      0.8989,     0.89854,     0.89796,     0.89774,     0.89753,     0.89753,     0.89734,     0.89652,     0.89651,     0.89651,      0.8962,     0.89615,     0.89563,\n            0.89537,     0.89473,     0.89448,     0.89365,     0.89365,     0.89314,     0.89255,     0.89217,     0.89205,     0.89186,     0.89086,     0.89078,     0.89078,     0.89078,     0.89072,     0.89072,     0.89068,     0.89068,     0.88989,     0.88989,     0.88989,     0.88924,     0.88918,\n            0.88882,     0.88861,     0.88802,     0.88768,     0.88761,     0.88761,     0.88703,     0.88692,     0.88683,     0.88588,     0.88571,     0.88553,     0.88511,      0.8848,     0.88436,     0.88423,     0.88421,     0.88403,     0.88382,     0.88338,     0.88314,     0.88302,     0.88266,\n            0.88266,      0.8824,     0.88189,      0.8816,     0.88149,     0.88149,     0.88134,     0.88106,     0.88048,      0.8801,        0.88,     0.87974,     0.87974,     0.87974,     0.87974,     0.87973,      0.8795,     0.87939,     0.87933,     0.87895,     0.87845,     0.87808,     0.87775,\n            0.87762,     0.87705,     0.87679,     0.87638,     0.87568,     0.87563,     0.87538,     0.87523,     0.87515,      0.8748,     0.87462,     0.87443,     0.87414,     0.87394,     0.87355,     0.87321,     0.87273,     0.87273,     0.87269,     0.87269,      0.8726,      0.8726,     0.87214,\n            0.87165,     0.87134,     0.87105,     0.87066,     0.87017,     0.86998,     0.86956,     0.86933,     0.86924,     0.86924,     0.86879,     0.86857,     0.86857,     0.86845,     0.86815,     0.86801,     0.86756,     0.86756,     0.86708,     0.86689,     0.86689,     0.86676,     0.86636,\n            0.86576,     0.86533,     0.86482,     0.86437,     0.86388,     0.86345,     0.86273,     0.86209,     0.86194,     0.86192,     0.86185,     0.86165,      0.8608,     0.86029,     0.85967,     0.85817,     0.85736,     0.85701,     0.85649,      0.8564,      0.8564,     0.85624,     0.85587,\n            0.85531,      0.8545,     0.85416,     0.85391,      0.8534,     0.85286,     0.85255,     0.85238,     0.85181,     0.85137,      0.8513,     0.85127,     0.85091,     0.85058,     0.85046,     0.84966,     0.84939,     0.84914,     0.84898,     0.84891,     0.84891,     0.84867,      0.8483,\n             0.8477,     0.84712,     0.84701,     0.84691,     0.84628,     0.84602,     0.84602,     0.84559,      0.8453,     0.84462,     0.84446,     0.84404,     0.84404,     0.84382,     0.84323,     0.84315,     0.84306,     0.84256,     0.84248,     0.84218,     0.84176,     0.84132,     0.84132,\n            0.84132,     0.84095,     0.84068,     0.84053,     0.84035,     0.84014,     0.84002,     0.83918,     0.83898,     0.83848,     0.83769,      0.8372,     0.83662,     0.83651,     0.83595,     0.83497,     0.83472,     0.83457,     0.83428,     0.83409,     0.83331,      0.8331,     0.83226,\n            0.83196,     0.83138,     0.83081,     0.82991,     0.82971,     0.82905,     0.82837,     0.82825,     0.82759,      0.8273,     0.82698,     0.82665,     0.82637,     0.82615,     0.82548,     0.82511,     0.82472,     0.82436,     0.82371,     0.82287,     0.82236,     0.82205,      0.8216,\n            0.82081,     0.81977,     0.81892,     0.81869,     0.81811,     0.81739,     0.81708,     0.81668,     0.81609,     0.81586,     0.81534,     0.81486,     0.81457,     0.81417,     0.81341,     0.81295,     0.81261,     0.81222,     0.81192,     0.81158,     0.81133,     0.81118,     0.81074,\n            0.81017,     0.81007,      0.8097,     0.80922,     0.80889,     0.80855,     0.80745,     0.80705,     0.80648,     0.80616,     0.80575,     0.80528,     0.80473,     0.80409,     0.80397,     0.80369,     0.80349,     0.80346,     0.80237,     0.80201,     0.80157,     0.80132,     0.80109,\n            0.80097,     0.80025,     0.79957,      0.7993,     0.79895,     0.79853,     0.79818,     0.79812,     0.79774,      0.7975,     0.79707,     0.79684,     0.79625,     0.79612,      0.7958,     0.79543,     0.79496,     0.79467,     0.79425,       0.794,     0.79358,      0.7933,       0.793,\n            0.79278,      0.7924,      0.7918,     0.79113,     0.79065,     0.79003,      0.7896,     0.78948,     0.78864,     0.78792,     0.78745,     0.78675,     0.78583,     0.78562,     0.78522,     0.78466,     0.78409,     0.78352,     0.78318,      0.7827,     0.78212,     0.78104,     0.78072,\n            0.78021,     0.77985,     0.77936,       0.779,     0.77848,     0.77747,     0.77645,     0.77554,     0.77495,     0.77417,     0.77375,     0.77306,     0.77286,     0.77167,      0.7709,      0.7704,     0.76945,     0.76902,     0.76831,     0.76769,     0.76646,     0.76612,     0.76534,\n            0.76483,     0.76436,     0.76352,     0.76305,     0.76206,     0.76153,     0.76073,     0.76049,     0.75966,     0.75866,     0.75799,     0.75731,     0.75669,     0.75634,     0.75617,     0.75549,     0.75499,     0.75439,      0.7537,     0.75318,     0.75269,     0.75189,     0.75065,\n            0.74934,     0.74832,      0.7475,     0.74697,     0.74613,      0.7444,     0.74313,     0.74226,     0.74155,     0.74077,     0.74028,     0.73939,     0.73805,     0.73755,     0.73663,     0.73582,     0.73525,      0.7343,     0.73303,      0.7323,     0.73139,     0.73017,     0.72882,\n            0.72783,     0.72681,     0.72594,     0.72526,     0.72383,      0.7233,     0.72279,     0.72179,     0.72068,     0.71958,     0.71907,     0.71767,     0.71709,     0.71588,      0.7147,     0.71378,     0.71223,     0.71138,     0.71072,     0.70994,     0.70915,     0.70824,     0.70715,\n            0.70623,     0.70522,     0.70404,     0.70318,     0.70224,     0.70097,     0.69969,     0.69904,     0.69774,     0.69668,      0.6956,     0.69505,     0.69384,     0.69291,     0.69246,     0.69187,     0.69032,     0.68912,     0.68805,     0.68723,     0.68561,     0.68446,     0.68348,\n            0.68266,     0.68143,     0.68022,     0.67936,     0.67795,     0.67697,     0.67574,     0.67451,     0.67368,     0.67258,     0.67111,     0.67039,      0.6696,     0.66828,     0.66646,     0.66567,     0.66515,     0.66409,     0.66344,     0.66189,     0.66039,     0.65894,      0.6576,\n            0.65659,     0.65524,     0.65399,     0.65292,     0.65136,     0.65037,     0.64917,     0.64786,     0.64637,     0.64576,     0.64498,     0.64353,     0.64222,     0.64131,     0.64018,     0.63913,     0.63859,     0.63739,     0.63653,     0.63507,     0.63411,     0.63231,      0.6305,\n            0.62926,     0.62794,     0.62658,     0.62493,     0.62279,     0.62129,     0.61919,     0.61733,     0.61554,     0.61388,     0.61218,     0.61088,     0.60964,     0.60818,     0.60678,     0.60542,     0.60374,     0.60254,     0.60097,     0.59996,     0.59817,     0.59623,     0.59439,\n            0.59298,     0.59118,     0.58979,     0.58747,     0.58598,     0.58384,     0.58227,     0.58058,       0.579,     0.57741,     0.57524,      0.5734,     0.57096,     0.56877,      0.5668,     0.56462,      0.5631,      0.5618,     0.56004,     0.55869,     0.55663,     0.55498,      0.5526,\n            0.55069,     0.54814,     0.54638,     0.54313,     0.54108,     0.53893,     0.53754,     0.53558,     0.53359,     0.53098,     0.52865,     0.52691,     0.52528,     0.52313,     0.52091,     0.51897,     0.51606,     0.51408,     0.51184,     0.50952,     0.50706,     0.50477,     0.50327,\n            0.49999,     0.49809,     0.49576,     0.49358,     0.49109,     0.48787,     0.48596,     0.48328,     0.48113,     0.47874,     0.47744,     0.47391,     0.47095,     0.46918,     0.46745,     0.46396,     0.46148,     0.45834,     0.45514,     0.45314,     0.44947,     0.44751,     0.44393,\n            0.44031,     0.43704,     0.43373,     0.43073,     0.42919,     0.42619,     0.42159,     0.41886,     0.41495,     0.41316,     0.40975,     0.40638,     0.40392,     0.40077,     0.39759,     0.39484,     0.39227,     0.38981,     0.38702,      0.3839,     0.38192,     0.37767,     0.37495,\n            0.37187,     0.36865,     0.36644,     0.36129,     0.35728,     0.35465,     0.35123,     0.34835,     0.34511,     0.34234,     0.33852,     0.33482,     0.33174,     0.32659,     0.32282,     0.31938,     0.31644,     0.31337,     0.31102,     0.30678,     0.30419,     0.30137,     0.29734,\n            0.29448,     0.29144,     0.28773,     0.28471,     0.28046,     0.27638,     0.27482,     0.27194,      0.2684,     0.26539,     0.26199,      0.2572,     0.25373,     0.25088,     0.24699,     0.24319,     0.23962,     0.23717,     0.23219,     0.22906,     0.22622,      0.2223,     0.21872,\n              0.215,     0.21102,     0.20777,     0.20418,     0.20155,     0.19902,     0.19613,     0.19334,     0.19007,     0.18815,     0.18557,     0.18342,     0.17916,     0.17757,     0.17446,     0.17049,     0.16758,     0.16497,     0.16109,     0.15822,     0.15522,     0.15178,     0.14862,\n            0.14411,     0.14132,     0.13735,     0.13444,     0.13086,     0.12816,     0.12563,      0.1223,     0.11928,     0.11653,     0.11289,     0.11044,     0.10793,     0.10492,     0.10207,    0.099661,    0.096737,    0.093851,    0.091847,     0.08883,    0.086074,    0.083973,     0.08218,\n            0.08004,    0.078015,    0.075204,    0.072511,    0.069868,     0.06718,    0.064492,    0.061701,    0.059704,    0.057816,    0.055839,    0.053415,    0.051289,    0.048963,    0.046444,    0.044149,    0.042477,    0.040588,    0.039087,    0.037869,    0.036429,    0.034439,    0.032492,\n           0.029911,    0.027921,    0.026302,    0.025205,    0.023112,    0.021558,    0.020207,    0.019074,    0.017746,    0.016658,     0.01581,    0.014599,     0.01351,    0.012798,     0.01171,    0.011107,     0.01049,   0.0098727,   0.0092556,   0.0086386,   0.0080215,   0.0074045,   0.0067875,\n          0.0061704,   0.0055534,   0.0049363,   0.0043193,   0.0037022,   0.0030852,   0.0024682,   0.0018511,   0.0012341,  0.00061704,           0]]), 'Recall', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[   0.022727,    0.022741,    0.034343,    0.044472,    0.054053,    0.063357,    0.072157,     0.08054,    0.088565,    0.096382,     0.10403,     0.11128,     0.11829,      0.1253,       0.132,     0.13863,     0.14522,     0.15157,     0.15783,     0.16413,     0.17029,     0.17625,     0.18212,\n            0.18796,     0.19377,     0.19944,     0.20491,     0.21046,     0.21595,     0.22133,     0.22636,     0.23159,     0.23673,     0.24158,     0.24669,     0.25161,     0.25651,     0.26124,     0.26595,     0.27067,     0.27535,     0.27991,     0.28431,     0.28869,     0.29303,      0.2972,\n            0.30132,     0.30545,     0.30947,     0.31349,     0.31755,     0.32161,     0.32531,     0.32912,     0.33296,     0.33666,     0.34058,     0.34426,     0.34793,     0.35171,     0.35523,     0.35893,     0.36238,     0.36596,     0.36943,     0.37291,     0.37619,     0.37956,     0.38266,\n             0.3862,     0.38936,     0.39263,      0.3956,     0.39875,     0.40224,     0.40552,     0.40879,     0.41199,     0.41507,     0.41798,     0.42065,     0.42359,     0.42645,     0.42939,     0.43244,     0.43529,     0.43806,     0.44081,     0.44327,      0.4463,     0.44889,     0.45167,\n            0.45428,     0.45715,     0.45965,     0.46244,     0.46495,      0.4676,     0.47023,     0.47275,     0.47528,     0.47798,     0.48047,     0.48289,     0.48537,     0.48753,     0.49016,     0.49228,     0.49466,     0.49685,     0.49932,     0.50167,     0.50413,     0.50635,     0.50871,\n            0.51105,     0.51341,     0.51528,     0.51743,      0.5196,     0.52143,     0.52385,     0.52614,     0.52822,     0.53034,     0.53232,     0.53417,     0.53574,     0.53769,      0.5397,     0.54173,     0.54366,     0.54573,      0.5475,     0.54939,     0.55131,     0.55331,     0.55484,\n            0.55692,     0.55886,     0.56082,     0.56264,     0.56442,     0.56626,     0.56767,     0.56963,     0.57134,     0.57327,     0.57482,     0.57661,     0.57822,     0.57995,     0.58135,     0.58289,     0.58474,     0.58592,     0.58762,     0.58917,     0.59085,     0.59227,     0.59385,\n             0.5954,      0.5967,     0.59791,      0.5994,     0.60104,     0.60235,     0.60351,     0.60481,       0.606,     0.60719,     0.60864,     0.60992,     0.61139,     0.61252,     0.61382,     0.61518,     0.61633,     0.61764,     0.61899,     0.61998,     0.62101,     0.62187,     0.62312,\n            0.62418,     0.62527,     0.62648,      0.6277,     0.62877,     0.62973,      0.6309,     0.63204,      0.6328,     0.63379,     0.63468,     0.63581,      0.6368,     0.63769,     0.63863,     0.63957,     0.64048,     0.64133,     0.64221,     0.64345,     0.64456,     0.64525,     0.64622,\n            0.64709,     0.64787,     0.64876,      0.6496,     0.65042,     0.65095,     0.65172,     0.65242,     0.65289,     0.65374,     0.65444,     0.65523,     0.65606,     0.65673,     0.65759,     0.65834,     0.65913,     0.65991,     0.66041,     0.66099,     0.66155,     0.66211,     0.66291,\n            0.66357,     0.66442,     0.66502,     0.66563,     0.66621,     0.66659,     0.66742,     0.66803,      0.6685,     0.66897,     0.66927,     0.66988,     0.67025,     0.67079,     0.67122,     0.67158,     0.67197,     0.67239,      0.6728,     0.67334,     0.67391,      0.6742,     0.67478,\n            0.67527,     0.67563,     0.67625,     0.67666,     0.67723,     0.67773,     0.67809,     0.67836,     0.67898,     0.67918,     0.67976,     0.68026,     0.68038,     0.68083,     0.68085,     0.68105,     0.68105,     0.68116,     0.68128,     0.68148,     0.68185,     0.68209,     0.68197,\n            0.68225,     0.68258,     0.68281,     0.68291,     0.68321,      0.6834,     0.68335,      0.6839,     0.68408,     0.68418,     0.68439,     0.68457,     0.68496,     0.68526,     0.68521,     0.68509,     0.68507,     0.68511,     0.68556,     0.68567,     0.68565,     0.68557,     0.68579,\n            0.68598,     0.68586,     0.68605,     0.68624,     0.68623,     0.68639,     0.68638,     0.68657,     0.68661,     0.68668,     0.68665,     0.68662,     0.68692,     0.68681,     0.68693,     0.68699,     0.68714,     0.68702,     0.68665,     0.68662,     0.68674,     0.68639,     0.68652,\n             0.6866,     0.68662,     0.68638,     0.68654,     0.68652,     0.68654,     0.68656,     0.68641,     0.68636,     0.68626,     0.68633,     0.68638,     0.68605,     0.68595,     0.68563,     0.68551,     0.68566,     0.68561,     0.68558,     0.68548,     0.68551,     0.68527,      0.6851,\n            0.68512,     0.68497,     0.68495,     0.68483,     0.68453,     0.68415,     0.68425,     0.68399,     0.68383,     0.68373,     0.68362,     0.68358,     0.68355,     0.68357,     0.68346,     0.68314,     0.68323,     0.68296,     0.68273,     0.68236,     0.68223,     0.68189,     0.68204,\n            0.68177,     0.68142,      0.6811,     0.68063,     0.68048,     0.68046,     0.68061,     0.68037,     0.68025,     0.67977,     0.67961,      0.6796,     0.67947,     0.67939,     0.67915,     0.67878,     0.67834,     0.67801,     0.67759,     0.67694,     0.67635,     0.67566,     0.67535,\n            0.67491,     0.67463,     0.67445,     0.67404,     0.67368,     0.67314,     0.67279,     0.67233,     0.67207,     0.67153,     0.67105,     0.67079,     0.67025,     0.66985,      0.6694,     0.66889,     0.66872,     0.66817,     0.66771,     0.66758,      0.6668,     0.66656,     0.66583,\n            0.66561,     0.66506,     0.66481,     0.66466,     0.66408,     0.66329,     0.66267,     0.66228,     0.66155,     0.66106,     0.66071,     0.66016,     0.65959,     0.65921,     0.65862,     0.65814,     0.65744,     0.65686,     0.65661,     0.65596,      0.6553,     0.65493,     0.65399,\n            0.65356,     0.65274,     0.65221,     0.65167,     0.65112,     0.65008,     0.64933,     0.64882,     0.64802,     0.64726,     0.64654,     0.64575,     0.64465,     0.64409,     0.64347,     0.64292,     0.64218,     0.64079,      0.6401,     0.63953,     0.63883,     0.63821,      0.6377,\n            0.63709,     0.63618,     0.63525,     0.63449,     0.63394,     0.63294,     0.63218,     0.63166,     0.63105,     0.63053,     0.62993,     0.62943,     0.62874,     0.62815,      0.6276,     0.62696,     0.62622,     0.62548,     0.62495,     0.62444,     0.62357,     0.62292,     0.62244,\n             0.6217,     0.62083,     0.62004,     0.61929,     0.61858,     0.61779,     0.61707,     0.61653,     0.61582,     0.61483,     0.61422,     0.61344,     0.61261,     0.61188,     0.61136,     0.61088,     0.61011,     0.60946,     0.60899,     0.60851,     0.60801,      0.6073,     0.60633,\n            0.60552,     0.60496,     0.60418,      0.6035,     0.60259,     0.60195,      0.6011,     0.60028,     0.59963,     0.59867,     0.59803,     0.59718,     0.59631,     0.59558,     0.59478,     0.59422,     0.59362,     0.59279,     0.59191,      0.5914,     0.59026,     0.58971,      0.5891,\n            0.58837,     0.58736,     0.58652,     0.58574,     0.58489,     0.58409,     0.58319,     0.58265,     0.58143,     0.58074,      0.5796,      0.5787,       0.578,      0.5768,     0.57582,     0.57475,     0.57385,     0.57272,     0.57165,     0.57035,     0.56941,     0.56807,     0.56753,\n            0.56646,     0.56528,     0.56457,     0.56377,     0.56282,     0.56175,     0.56071,     0.55991,     0.55929,     0.55782,     0.55694,     0.55621,     0.55533,     0.55444,     0.55334,     0.55225,     0.55122,     0.55023,     0.54935,     0.54847,     0.54762,      0.5469,     0.54609,\n            0.54473,     0.54374,     0.54234,     0.54136,     0.54041,     0.53938,     0.53872,     0.53784,     0.53708,     0.53618,     0.53502,     0.53384,     0.53269,     0.53167,     0.53073,     0.52985,     0.52945,     0.52831,     0.52722,     0.52612,      0.5253,     0.52391,     0.52295,\n            0.52189,     0.52064,     0.51968,     0.51866,     0.51744,     0.51626,     0.51535,     0.51417,      0.5125,     0.51146,     0.51041,     0.50907,     0.50763,     0.50634,      0.5051,     0.50378,     0.50263,     0.50134,     0.50053,     0.49956,     0.49818,     0.49729,     0.49569,\n            0.49457,     0.49349,     0.49252,     0.49114,     0.48972,     0.48848,     0.48677,     0.48608,       0.485,     0.48325,     0.48231,     0.48087,     0.47958,     0.47828,     0.47719,     0.47557,     0.47447,      0.4732,     0.47221,     0.47153,     0.47021,     0.46901,     0.46774,\n            0.46632,     0.46488,     0.46343,     0.46216,     0.46041,     0.45876,     0.45766,     0.45659,     0.45517,     0.45398,     0.45257,      0.4512,     0.44975,     0.44836,      0.4473,     0.44578,     0.44408,     0.44287,     0.44101,     0.43936,     0.43741,     0.43585,     0.43442,\n            0.43323,     0.43178,     0.43025,     0.42848,     0.42765,     0.42592,     0.42477,     0.42382,     0.42258,     0.42153,     0.41988,     0.41839,     0.41634,      0.4153,     0.41394,     0.41276,     0.41095,     0.40862,     0.40675,     0.40535,     0.40434,     0.40265,     0.40166,\n            0.39997,      0.3988,     0.39738,     0.39595,     0.39453,     0.39314,     0.39179,      0.3903,     0.38892,     0.38727,       0.386,     0.38442,     0.38307,     0.38155,      0.3801,     0.37854,     0.37698,     0.37512,     0.37324,     0.37166,     0.37006,     0.36846,      0.3671,\n            0.36544,     0.36396,     0.36248,     0.36073,     0.35927,     0.35744,       0.355,     0.35344,     0.35192,     0.35033,     0.34865,     0.34705,     0.34529,     0.34375,     0.34182,     0.34009,     0.33802,     0.33626,     0.33445,     0.33252,     0.33087,     0.32953,     0.32735,\n            0.32542,     0.32381,     0.32238,     0.32074,     0.31875,     0.31721,     0.31494,     0.31378,     0.31183,     0.31012,     0.30825,     0.30622,     0.30419,      0.3019,     0.30015,     0.29818,     0.29648,     0.29465,     0.29225,      0.2906,     0.28897,     0.28706,     0.28544,\n            0.28304,     0.28133,     0.27906,     0.27705,     0.27496,     0.27268,     0.27045,     0.26855,      0.2673,     0.26496,     0.26327,     0.26118,     0.25944,     0.25797,     0.25562,     0.25426,     0.25224,     0.25045,     0.24866,     0.24715,     0.24547,     0.24431,     0.24238,\n            0.24068,     0.23858,     0.23625,     0.23426,     0.23258,     0.23013,     0.22861,     0.22731,     0.22572,     0.22324,     0.22148,     0.21986,     0.21841,      0.2157,     0.21399,      0.2125,     0.21053,     0.20935,     0.20763,     0.20537,     0.20385,     0.20155,     0.19992,\n            0.19837,     0.19651,     0.19424,     0.19207,     0.19019,     0.18692,     0.18533,     0.18348,      0.1812,     0.17951,     0.17803,     0.17607,     0.17427,     0.17201,     0.17021,     0.16779,     0.16665,     0.16463,     0.16336,     0.16115,      0.1595,     0.15787,     0.15544,\n            0.15346,     0.15153,     0.14994,      0.1479,     0.14635,     0.14403,     0.14245,     0.14063,     0.13872,     0.13658,     0.13467,     0.13291,      0.1315,     0.12907,     0.12744,       0.125,     0.12322,     0.12135,     0.11952,     0.11797,     0.11636,     0.11453,     0.11293,\n            0.11111,     0.10951,     0.10802,     0.10569,     0.10389,     0.10266,     0.10121,    0.098924,    0.097258,    0.095849,    0.094466,    0.092497,    0.091112,    0.089566,    0.087719,    0.086551,    0.085658,    0.083985,    0.082318,    0.081008,    0.079319,    0.077428,     0.07558,\n           0.073966,    0.072506,    0.070824,    0.069474,    0.068149,    0.066663,    0.065341,    0.064718,    0.063295,    0.061689,    0.059491,    0.057513,    0.056493,    0.055374,     0.05392,     0.05298,    0.051682,    0.050987,      0.0495,    0.048851,    0.047861,    0.046367,    0.045483,\n           0.043729,    0.042923,    0.041474,    0.039998,    0.039421,    0.038128,    0.037392,    0.036641,    0.034266,    0.033574,    0.032766,     0.03155,    0.030735,     0.02999,    0.028862,    0.028231,    0.027431,    0.026132,    0.024936,      0.0239,    0.023146,    0.022359,    0.020832,\n           0.019631,    0.019054,     0.01847,    0.017508,     0.01676,    0.016085,    0.015641,     0.01505,    0.014705,    0.014137,    0.013509,    0.012702,    0.012238,    0.011506,    0.011178,    0.010643,    0.010185,   0.0096798,   0.0093194,   0.0086052,   0.0081723,   0.0080376,   0.0078671,\n          0.0076033,   0.0072212,   0.0068461,   0.0065378,   0.0064326,   0.0059608,   0.0054881,   0.0049768,    0.004672,   0.0043872,   0.0040596,   0.0037755,   0.0035162,   0.0032461,   0.0031996,   0.0028412,   0.0027184,   0.0026001,   0.0024064,   0.0023125,   0.0022328,    0.002126,   0.0019948,\n          0.0019414,   0.0018663,   0.0018017,   0.0017673,   0.0017397,   0.0017121,   0.0015251,    0.001347,   0.0011351,  0.00099408,  0.00073263,  0.00073266,  0.00061392,  0.00052081,  0.00031048,  0.00021662,  0.00016022,  0.00014413,  0.00012803,  0.00011194,   9.584e-05,           0,           0,\n                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0]]), 'Confidence', 'F1'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[   0.011497,    0.011504,    0.017479,    0.022757,    0.027803,    0.032753,    0.037483,    0.042033,    0.046429,    0.050749,    0.055015,    0.059095,    0.063071,    0.067076,    0.070938,    0.074798,    0.078653,    0.082411,    0.086137,    0.089918,    0.093642,    0.097273,     0.10088,\n             0.1045,     0.10811,     0.11168,     0.11513,     0.11866,     0.12219,     0.12567,     0.12895,     0.13237,     0.13576,     0.13899,     0.14241,     0.14572,     0.14904,     0.15227,     0.15551,     0.15877,     0.16204,     0.16523,     0.16835,     0.17147,     0.17456,     0.17757,\n            0.18056,     0.18357,     0.18655,     0.18953,     0.19255,     0.19559,     0.19838,     0.20129,     0.20422,     0.20706,     0.21007,     0.21293,     0.21578,     0.21875,     0.22152,     0.22446,     0.22722,     0.23009,      0.2329,     0.23571,     0.23841,     0.24117,     0.24375,\n            0.24667,     0.24932,     0.25207,     0.25461,     0.25728,     0.26024,     0.26306,      0.2659,     0.26868,     0.27138,     0.27396,      0.2764,     0.27899,     0.28156,     0.28419,     0.28694,     0.28955,     0.29209,     0.29464,     0.29694,     0.29975,     0.30219,      0.3048,\n            0.30731,     0.31001,     0.31243,     0.31512,     0.31757,     0.32018,     0.32274,     0.32522,     0.32772,     0.33036,     0.33282,     0.33527,     0.33775,     0.33998,     0.34265,     0.34485,     0.34729,     0.34959,      0.3522,     0.35466,     0.35729,     0.35964,     0.36212,\n            0.36459,      0.3671,     0.36919,     0.37155,     0.37392,     0.37606,     0.37868,     0.38118,     0.38358,     0.38597,     0.38818,     0.39036,     0.39225,     0.39453,     0.39687,     0.39927,     0.40152,     0.40395,     0.40609,     0.40835,     0.41061,     0.41302,     0.41499,\n            0.41746,      0.4199,     0.42229,     0.42448,     0.42666,     0.42895,     0.43088,     0.43334,     0.43554,     0.43793,     0.43993,      0.4422,     0.44429,     0.44653,     0.44842,     0.45054,     0.45286,     0.45458,     0.45682,     0.45889,     0.46114,      0.4632,     0.46529,\n            0.46744,     0.46941,     0.47123,     0.47333,     0.47556,     0.47744,     0.47929,     0.48123,     0.48304,     0.48485,     0.48697,     0.48894,     0.49106,     0.49284,     0.49478,     0.49692,     0.49875,     0.50081,     0.50283,     0.50452,      0.5062,     0.50769,     0.50958,\n            0.51128,     0.51307,     0.51509,     0.51706,     0.51878,     0.52052,     0.52252,     0.52435,     0.52584,     0.52762,     0.52925,     0.53117,     0.53283,     0.53443,     0.53614,     0.53794,     0.53973,     0.54134,     0.54299,     0.54504,      0.5469,      0.5484,     0.55002,\n            0.55173,     0.55323,     0.55496,     0.55663,     0.55821,     0.55946,     0.56112,     0.56265,     0.56401,     0.56567,     0.56717,      0.5687,     0.57029,     0.57179,     0.57341,     0.57503,     0.57667,     0.57832,     0.57965,      0.5811,     0.58232,     0.58383,     0.58553,\n            0.58706,     0.58881,     0.59017,     0.59192,     0.59335,     0.59458,      0.5964,     0.59785,     0.59905,     0.60037,     0.60153,     0.60312,     0.60429,      0.6057,     0.60704,     0.60826,     0.60942,     0.61082,     0.61217,     0.61359,      0.6151,      0.6162,     0.61767,\n            0.61899,      0.6201,     0.62149,      0.6228,     0.62434,     0.62571,       0.627,     0.62817,     0.62975,     0.63086,     0.63258,     0.63395,     0.63504,     0.63658,     0.63745,     0.63858,     0.63937,     0.64031,     0.64158,     0.64282,     0.64416,     0.64535,     0.64629,\n            0.64744,     0.64904,      0.6502,     0.65115,     0.65262,     0.65367,      0.6545,     0.65621,      0.6574,     0.65846,     0.65952,      0.6606,     0.66188,     0.66325,     0.66435,     0.66511,     0.66588,     0.66675,     0.66815,     0.66932,     0.67026,     0.67115,     0.67232,\n            0.67339,     0.67423,     0.67573,     0.67682,     0.67784,     0.67897,     0.68021,     0.68139,     0.68236,     0.68339,     0.68434,     0.68549,     0.68679,     0.68771,     0.68897,     0.69001,     0.69119,     0.69223,       0.693,     0.69404,     0.69531,     0.69601,     0.69714,\n            0.69808,     0.69903,     0.69972,     0.70107,     0.70192,     0.70289,     0.70384,     0.70456,     0.70579,     0.70678,     0.70766,     0.70887,     0.70947,     0.71056,     0.71114,     0.71193,     0.71315,     0.71417,     0.71497,     0.71596,     0.71674,     0.71743,     0.71818,\n            0.71922,     0.72031,     0.72127,     0.72245,     0.72318,     0.72385,     0.72509,     0.72596,     0.72677,     0.72773,     0.72825,     0.72936,     0.73052,     0.73138,     0.73229,     0.73296,     0.73414,     0.73491,      0.7358,     0.73674,     0.73748,       0.738,      0.7392,\n            0.74001,     0.74058,     0.74146,     0.74226,     0.74325,     0.74431,     0.74542,     0.74638,     0.74735,     0.74809,     0.74896,      0.7498,     0.75068,     0.75173,     0.75224,     0.75289,     0.75369,     0.75433,     0.75505,     0.75547,     0.75628,      0.7567,     0.75749,\n            0.75818,     0.75916,     0.76004,     0.76049,     0.76129,     0.76169,     0.76253,     0.76344,     0.76435,     0.76484,     0.76543,     0.76643,     0.76712,     0.76817,     0.76889,     0.76941,     0.77028,      0.7709,     0.77168,     0.77288,     0.77317,     0.77411,     0.77441,\n             0.7754,     0.77594,     0.77693,     0.77793,     0.77888,     0.77929,     0.77988,      0.7804,     0.78093,     0.78181,     0.78255,     0.78313,     0.78356,     0.78406,     0.78483,      0.7853,     0.78576,     0.78618,      0.7872,     0.78787,     0.78862,     0.78945,     0.78969,\n             0.7906,     0.79112,     0.79178,     0.79237,     0.79277,      0.7932,      0.7935,     0.79407,     0.79435,     0.79493,     0.79541,     0.79574,     0.79623,     0.79681,     0.79703,     0.79746,     0.79812,     0.79812,     0.79867,     0.79923,     0.79937,     0.79992,     0.80059,\n            0.80107,     0.80143,      0.8018,      0.8023,     0.80321,     0.80346,     0.80379,     0.80406,     0.80423,      0.8047,     0.80527,     0.80576,      0.8062,     0.80671,     0.80729,     0.80802,     0.80868,     0.80918,     0.80946,     0.80993,     0.81014,     0.81061,     0.81096,\n            0.81131,     0.81153,     0.81192,     0.81218,     0.81268,     0.81319,     0.81361,      0.8143,     0.81478,     0.81515,     0.81584,     0.81616,     0.81687,     0.81726,     0.81763,     0.81815,     0.81866,     0.81907,     0.81978,     0.82084,     0.82155,     0.82203,     0.82237,\n            0.82355,     0.82428,     0.82454,     0.82507,     0.82543,     0.82613,     0.82629,     0.82667,     0.82708,     0.82716,     0.82754,     0.82834,     0.82886,     0.82961,     0.82989,     0.83079,     0.83133,     0.83204,     0.83245,     0.83305,      0.8333,     0.83425,     0.83437,\n            0.83449,     0.83478,     0.83557,     0.83631,     0.83657,     0.83703,     0.83771,     0.83859,     0.83893,     0.83981,     0.84015,     0.84036,     0.84064,     0.84074,     0.84121,     0.84117,     0.84124,     0.84203,     0.84243,     0.84251,     0.84307,     0.84324,     0.84377,\n            0.84399,     0.84405,     0.84446,     0.84509,     0.84537,     0.84585,     0.84601,     0.84642,      0.8466,     0.84704,     0.84749,     0.84819,     0.84848,     0.84879,     0.84874,     0.84904,     0.84909,     0.84948,     0.85018,     0.85053,     0.85071,      0.8511,     0.85118,\n            0.85136,     0.85181,     0.85226,      0.8528,      0.8534,     0.85377,     0.85413,     0.85442,     0.85518,     0.85582,     0.85603,     0.85617,     0.85646,     0.85675,      0.8573,      0.8582,     0.85962,     0.86014,     0.86094,     0.86173,     0.86186,     0.86188,     0.86219,\n            0.86301,     0.86363,     0.86426,      0.8648,      0.8656,     0.86573,     0.86645,     0.86674,     0.86655,     0.86751,     0.86732,       0.868,     0.86842,     0.86855,      0.8685,     0.86901,     0.86913,     0.86926,     0.86952,     0.86997,     0.87006,      0.8708,     0.87125,\n            0.87176,     0.87212,     0.87252,     0.87262,     0.87249,     0.87269,     0.87307,     0.87353,     0.87384,     0.87429,     0.87454,     0.87474,     0.87507,     0.87513,     0.87557,     0.87559,     0.87636,     0.87681,     0.87698,     0.87761,     0.87806,     0.87845,     0.87864,\n            0.87926,     0.87921,     0.87955,     0.87965,     0.87951,      0.8795,     0.87996,     0.88003,     0.88039,     0.88132,      0.8813,     0.88139,     0.88166,     0.88176,     0.88265,      0.8826,     0.88302,     0.88338,     0.88397,      0.8842,     0.88426,     0.88474,     0.88517,\n            0.88567,     0.88585,     0.88673,     0.88678,     0.88729,     0.88741,     0.88762,     0.88801,     0.88853,     0.88878,      0.8892,     0.88988,     0.88974,     0.88989,     0.89052,     0.89052,     0.89068,     0.89022,      0.8906,     0.89174,     0.89185,     0.89225,     0.89293,\n            0.89303,     0.89354,     0.89423,     0.89467,     0.89545,     0.89597,     0.89608,     0.89637,      0.8965,     0.89691,     0.89742,     0.89718,     0.89762,     0.89777,     0.89858,     0.89893,     0.89981,     0.89993,     0.89965,     0.90055,      0.9009,     0.90168,      0.9017,\n             0.9019,     0.90232,     0.90273,     0.90236,     0.90285,     0.90292,     0.90297,     0.90413,     0.90486,     0.90499,     0.90645,     0.90628,      0.9073,     0.90777,     0.90822,     0.90809,     0.90748,     0.90743,     0.90736,     0.90758,     0.90773,     0.90814,      0.9078,\n            0.90837,     0.90838,     0.90895,     0.90929,      0.9097,     0.91002,     0.90974,     0.91029,     0.91052,     0.91088,     0.91101,     0.91074,     0.91046,     0.91028,     0.91064,     0.91113,     0.91152,     0.91207,     0.91187,     0.91249,     0.91332,      0.9133,     0.91376,\n            0.91418,     0.91423,      0.9141,     0.91453,      0.9144,     0.91426,     0.91499,     0.91521,     0.91557,     0.91552,     0.91626,     0.91577,     0.91622,     0.91646,     0.91725,     0.91747,     0.91747,     0.91755,     0.91811,     0.91829,     0.91866,     0.91874,     0.91927,\n            0.91991,      0.9198,     0.91935,     0.91939,     0.91981,     0.92047,     0.92045,      0.9205,     0.91992,     0.91954,     0.91914,     0.91853,     0.91907,     0.91857,     0.91875,     0.91873,      0.9188,      0.9192,     0.91938,     0.91848,     0.91874,     0.91809,     0.91862,\n            0.91981,     0.91935,      0.9187,     0.91809,     0.91758,     0.91645,     0.91671,     0.91686,     0.91682,     0.91773,     0.91773,      0.9189,     0.91948,     0.91949,     0.91973,     0.91894,      0.9195,     0.92002,     0.92125,     0.92139,     0.92175,     0.92253,     0.92292,\n              0.924,     0.92388,     0.92349,     0.92502,     0.92509,     0.92611,     0.92709,     0.92706,     0.92755,     0.92774,     0.92864,     0.92916,      0.9289,     0.92757,     0.92864,     0.92783,     0.92838,     0.92837,     0.92783,     0.93023,      0.9304,      0.9316,     0.93065,\n            0.93013,     0.92972,      0.9306,     0.92973,     0.92917,     0.92898,      0.9306,      0.9297,     0.92989,     0.92958,     0.92954,     0.93069,     0.92966,     0.93072,      0.9293,     0.92914,     0.92999,     0.92864,     0.92886,      0.9294,     0.92795,     0.92711,     0.92628,\n            0.92473,      0.9242,     0.92341,     0.92199,     0.92152,     0.92085,     0.92034,     0.91961,     0.91894,     0.91801,     0.91737,     0.91699,     0.91674,     0.91635,     0.91545,     0.91654,     0.91713,      0.9213,     0.92042,      0.9208,     0.91925,     0.91825,     0.91822,\n            0.91973,     0.92302,     0.92375,      0.9211,     0.92517,     0.92581,     0.92681,     0.92727,     0.93458,     0.93538,     0.93387,     0.93145,     0.93426,     0.93271,     0.93505,     0.93616,     0.93439,     0.93399,      0.9338,     0.93402,     0.93199,     0.93286,     0.93494,\n             0.9312,      0.9329,     0.93853,     0.93534,     0.93263,     0.92998,     0.92812,     0.92549,     0.92729,     0.94083,     0.93823,     0.93454,      0.9322,     0.93428,     0.93246,     0.93591,     0.94018,     0.93724,     0.93692,     0.93813,     0.94381,     0.94292,     0.94471,\n            0.94936,     0.95699,     0.95473,     0.95269,     0.95195,     0.94833,     0.94412,     0.93871,     0.93496,     0.93103,     0.92586,     0.92071,     0.93514,     0.93011,     0.92916,     0.92092,     0.91763,     0.91421,     0.90792,     0.90453,     0.90145,     0.89702,     0.89095,\n            0.88831,     0.88434,     0.88067,     0.87861,     0.87691,     0.87521,     0.86197,     0.84647,     0.82294,     0.80267,      0.7566,     0.80765,      0.7898,     0.76103,     0.79065,     0.72168,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1]]), 'Confidence', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.98135,     0.98135,     0.97516,     0.97129,     0.96811,     0.96579,     0.96315,     0.96022,     0.95802,     0.95614,     0.95427,      0.9524,     0.95032,     0.94902,     0.94767,     0.94608,     0.94462,     0.94258,     0.94091,     0.93957,      0.9381,     0.93684,     0.93546,\n            0.93399,     0.93289,     0.93138,     0.93028,     0.92939,     0.92796,      0.9269,     0.92568,     0.92458,     0.92365,     0.92234,     0.92161,     0.92051,      0.9197,      0.9188,     0.91778,     0.91681,     0.91567,     0.91481,     0.91367,     0.91265,     0.91188,      0.9109,\n             0.9096,     0.90891,      0.9074,     0.90618,     0.90512,      0.9043,      0.9032,     0.90198,     0.90088,     0.89999,      0.8993,      0.8982,     0.89767,     0.89689,     0.89608,     0.89535,     0.89441,     0.89364,      0.8927,     0.89229,     0.89127,     0.89054,     0.88973,\n            0.88907,      0.8883,     0.88761,     0.88651,     0.88586,     0.88533,     0.88455,      0.8837,     0.88284,     0.88219,      0.8813,     0.87987,      0.8793,     0.87849,     0.87796,     0.87731,     0.87641,     0.87572,     0.87478,     0.87393,     0.87327,     0.87242,     0.87169,\n            0.87071,     0.87018,     0.86928,     0.86839,     0.86753,      0.8666,     0.86599,     0.86521,     0.86456,     0.86411,     0.86358,     0.86279,      0.8622,     0.86138,     0.86073,     0.85992,     0.85931,     0.85849,     0.85756,     0.85682,     0.85585,     0.85523,     0.85471,\n            0.85418,     0.85361,     0.85267,     0.85194,     0.85124,     0.85002,     0.84953,     0.84905,     0.84799,     0.84721,     0.84672,     0.84579,     0.84478,     0.84391,     0.84314,     0.84229,     0.84159,     0.84082,        0.84,     0.83927,      0.8387,     0.83793,     0.83683,\n             0.8363,     0.83532,     0.83463,     0.83414,     0.83357,     0.83288,     0.83174,     0.83101,     0.83019,      0.8297,     0.82901,      0.8284,     0.82771,      0.8271,     0.82632,     0.82535,     0.82502,     0.82396,     0.82335,     0.82274,     0.82209,     0.82105,     0.82058,\n            0.81982,     0.81871,     0.81773,       0.817,     0.81645,     0.81578,     0.81468,     0.81378,     0.81293,     0.81211,     0.81138,     0.81044,     0.80983,     0.80898,     0.80829,     0.80731,     0.80645,     0.80555,     0.80494,     0.80397,     0.80319,      0.8023,     0.80177,\n            0.80108,     0.80026,     0.79932,     0.79859,     0.79794,     0.79692,     0.79603,     0.79541,     0.79437,     0.79346,     0.79257,     0.79179,     0.79118,     0.79041,     0.78956,     0.78853,     0.78747,     0.78662,      0.7858,     0.78523,     0.78466,     0.78365,      0.7832,\n             0.7823,     0.78157,     0.78073,     0.77986,     0.77913,     0.77823,     0.77721,     0.77628,     0.77501,     0.77428,     0.77347,     0.77281,     0.77218,     0.77131,     0.77074,     0.76988,     0.76911,     0.76829,     0.76732,     0.76634,     0.76573,     0.76463,     0.76386,\n              0.763,     0.76231,     0.76162,     0.76031,     0.75946,     0.75844,     0.75763,     0.75689,     0.75616,     0.75526,      0.7542,     0.75327,     0.75237,     0.75156,     0.75058,     0.74963,     0.74883,     0.74777,     0.74676,     0.74598,     0.74516,     0.74427,     0.74354,\n             0.7428,     0.74207,     0.74158,     0.74073,     0.73991,     0.73918,     0.73824,     0.73727,     0.73657,     0.73551,     0.73454,     0.73388,      0.7327,     0.73169,      0.7306,     0.72957,     0.72855,     0.72757,     0.72623,     0.72509,     0.72423,     0.72326,     0.72182,\n            0.72102,     0.71978,     0.71886,     0.71792,     0.71682,     0.71597,     0.71486,     0.71402,     0.71302,     0.71199,      0.7112,     0.71033,      0.7097,     0.70877,     0.70742,     0.70632,      0.7054,     0.70452,     0.70391,     0.70285,     0.70177,     0.70061,      0.6998,\n            0.69904,     0.69789,     0.69668,     0.69593,     0.69483,     0.69398,     0.69267,     0.69183,     0.69092,     0.68999,     0.68897,     0.68775,     0.68705,     0.68591,      0.6849,       0.684,     0.68315,     0.68189,     0.68042,     0.67936,     0.67837,     0.67704,     0.67621,\n            0.67549,     0.67464,     0.67354,      0.6726,     0.67178,     0.67093,     0.67011,     0.66918,     0.66798,      0.6669,     0.66625,     0.66527,     0.66413,     0.66299,     0.66188,     0.66098,     0.66022,     0.65924,     0.65851,     0.65749,     0.65688,     0.65586,     0.65493,\n            0.65411,     0.65293,     0.65212,     0.65093,      0.6498,     0.64857,     0.64776,      0.6466,     0.64568,     0.64474,     0.64413,     0.64321,     0.64226,     0.64162,     0.64073,     0.63965,     0.63892,     0.63786,      0.6368,     0.63546,     0.63469,     0.63371,     0.63309,\n            0.63203,     0.63101,     0.62982,     0.62846,     0.62749,     0.62671,     0.62618,     0.62509,     0.62421,     0.62288,     0.62202,     0.62142,      0.6206,     0.61974,     0.61901,     0.61795,     0.61669,     0.61571,     0.61455,      0.6132,     0.61171,     0.61029,     0.60928,\n            0.60813,     0.60704,     0.60618,     0.60524,     0.60415,     0.60302,     0.60195,     0.60064,     0.59967,     0.59851,     0.59739,     0.59637,     0.59511,     0.59384,     0.59271,      0.5916,     0.59083,      0.5896,     0.58843,     0.58754,     0.58617,     0.58525,     0.58395,\n            0.58305,     0.58191,     0.58097,     0.58018,     0.57877,     0.57735,     0.57609,     0.57522,     0.57383,     0.57263,     0.57169,     0.57056,     0.56949,     0.56866,     0.56737,     0.56643,     0.56515,     0.56408,     0.56318,     0.56188,     0.56053,     0.55958,     0.55809,\n            0.55701,     0.55556,     0.55447,      0.5534,     0.55241,     0.55072,      0.5495,     0.54849,     0.54722,     0.54586,      0.5446,     0.54334,     0.54156,      0.5405,     0.53952,     0.53855,     0.53721,     0.53528,     0.53406,     0.53301,     0.53199,     0.53089,     0.52988,\n            0.52883,     0.52742,       0.526,     0.52474,      0.5236,     0.52213,     0.52095,     0.52014,     0.51924,     0.51835,      0.5173,     0.51641,     0.51531,     0.51431,     0.51334,      0.5122,     0.51093,     0.50975,     0.50895,     0.50808,     0.50685,      0.5058,     0.50504,\n            0.50393,     0.50271,     0.50152,     0.50044,     0.49933,     0.49811,     0.49701,     0.49606,     0.49495,     0.49355,      0.4925,     0.49139,     0.49007,     0.48899,      0.4882,      0.4874,     0.48625,     0.48528,     0.48442,     0.48345,     0.48257,     0.48152,     0.48019,\n            0.47876,     0.47783,     0.47677,     0.47574,     0.47449,     0.47346,     0.47237,     0.47123,     0.47029,     0.46909,     0.46818,     0.46689,     0.46566,     0.46454,     0.46348,     0.46252,     0.46162,      0.4604,     0.45922,     0.45842,     0.45698,     0.45604,     0.45527,\n            0.45437,     0.45307,     0.45185,     0.45071,     0.44963,     0.44855,     0.44729,     0.44641,     0.44488,     0.44382,      0.4424,      0.4413,      0.4404,     0.43898,     0.43773,     0.43649,     0.43544,     0.43393,      0.4326,      0.4311,     0.42987,     0.42831,     0.42756,\n            0.42629,     0.42494,     0.42403,     0.42297,     0.42183,     0.42052,     0.41931,     0.41832,     0.41758,     0.41584,     0.41475,     0.41377,     0.41273,     0.41168,     0.41047,     0.40921,     0.40806,     0.40689,     0.40577,     0.40473,     0.40377,      0.4029,       0.402,\n            0.40049,     0.39932,     0.39771,     0.39655,     0.39539,     0.39421,     0.39343,     0.39244,     0.39147,     0.39038,      0.3891,     0.38783,     0.38656,     0.38543,     0.38433,     0.38323,     0.38252,     0.38124,     0.37994,     0.37865,     0.37778,     0.37633,     0.37529,\n            0.37404,     0.37264,     0.37154,     0.37041,     0.36902,      0.3678,     0.36674,     0.36549,     0.36385,     0.36263,     0.36161,     0.36015,     0.35864,     0.35733,      0.3561,     0.35471,     0.35354,     0.35225,     0.35141,     0.35038,     0.34901,     0.34801,     0.34638,\n            0.34521,      0.3441,      0.3431,     0.34174,     0.34039,     0.33916,     0.33746,     0.33673,     0.33565,      0.3339,     0.33297,     0.33158,      0.3303,     0.32906,     0.32797,     0.32643,     0.32529,     0.32404,     0.32309,     0.32237,     0.32108,     0.31991,      0.3187,\n             0.3173,     0.31597,     0.31459,     0.31341,     0.31182,     0.31031,     0.30925,     0.30827,     0.30693,     0.30574,     0.30446,     0.30321,     0.30187,     0.30061,     0.29955,     0.29819,     0.29663,     0.29551,     0.29379,      0.2923,     0.29058,     0.28915,     0.28784,\n            0.28674,     0.28546,     0.28403,     0.28249,     0.28171,      0.2802,     0.27918,     0.27833,     0.27721,     0.27628,     0.27483,     0.27349,     0.27175,     0.27085,     0.26964,     0.26864,     0.26709,     0.26517,     0.26356,     0.26229,     0.26143,     0.25999,     0.25911,\n             0.2577,     0.25668,     0.25545,     0.25423,       0.253,     0.25182,      0.2507,     0.24946,     0.24832,     0.24695,     0.24588,     0.24461,     0.24349,     0.24225,     0.24103,     0.23975,     0.23844,     0.23695,     0.23546,     0.23415,     0.23285,     0.23154,     0.23046,\n            0.22914,     0.22795,     0.22676,     0.22543,     0.22425,     0.22283,     0.22093,     0.21965,     0.21844,     0.21721,     0.21583,     0.21462,     0.21322,     0.21202,     0.21053,     0.20923,     0.20769,     0.20637,       0.205,     0.20355,     0.20231,     0.20128,     0.19968,\n            0.19821,     0.19702,     0.19594,     0.19471,     0.19323,     0.19208,     0.19044,     0.18956,     0.18813,     0.18687,     0.18551,     0.18405,      0.1826,     0.18096,     0.17969,     0.17826,     0.17703,     0.17571,     0.17401,     0.17282,     0.17164,      0.1703,     0.16914,\n            0.16744,     0.16624,     0.16466,     0.16325,     0.16181,     0.16024,     0.15867,     0.15737,     0.15649,      0.1549,     0.15372,     0.15231,     0.15112,     0.15011,      0.1485,     0.14758,     0.14622,     0.14502,     0.14381,     0.14279,     0.14166,     0.14089,     0.13959,\n            0.13845,     0.13707,     0.13554,     0.13423,     0.13312,      0.1315,     0.13051,     0.12967,     0.12864,     0.12704,     0.12591,     0.12488,     0.12393,     0.12219,      0.1211,     0.12015,     0.11888,     0.11813,     0.11703,     0.11561,     0.11464,      0.1132,     0.11217,\n            0.11117,     0.11001,      0.1086,     0.10725,     0.10609,     0.10407,     0.10309,     0.10194,     0.10054,    0.099488,    0.098575,     0.09736,    0.096254,    0.094877,    0.093784,    0.092326,    0.091627,    0.090402,    0.089628,    0.088297,    0.087303,    0.086322,    0.084864,\n           0.083676,    0.082535,    0.081592,    0.080378,    0.079461,    0.078089,    0.077152,    0.076087,    0.074968,    0.073714,    0.072597,    0.071575,     0.07076,    0.069361,    0.068412,    0.067015,    0.065986,    0.064918,    0.063873,     0.06298,    0.062061,    0.061013,    0.060113,\n           0.059087,    0.058182,    0.057337,    0.056029,    0.055022,     0.05433,    0.053516,    0.052241,    0.051312,     0.05053,    0.049762,    0.048667,    0.047903,    0.047047,    0.046032,    0.045389,    0.044897,    0.043981,    0.043067,     0.04235,     0.04143,    0.040401,    0.039397,\n           0.038523,    0.037733,    0.036824,    0.036097,    0.035383,    0.034583,    0.033873,    0.033539,    0.032776,    0.031917,    0.030742,    0.029688,    0.029145,     0.02855,    0.027778,    0.027278,     0.02659,    0.026219,    0.025434,    0.025091,     0.02457,    0.023784,    0.023319,\n           0.022397,    0.021972,    0.021213,    0.020443,    0.020139,    0.019465,    0.019081,     0.01869,    0.017453,    0.017094,    0.016676,    0.016047,    0.015625,     0.01524,    0.014657,    0.014331,     0.01392,    0.013252,    0.012637,    0.012105,    0.011718,    0.011315,    0.010534,\n          0.0099202,   0.0096252,   0.0093268,   0.0088366,   0.0084562,   0.0081128,   0.0078868,   0.0075867,   0.0074113,   0.0071219,   0.0068033,   0.0063945,   0.0061593,   0.0057886,   0.0056225,   0.0053521,   0.0051201,    0.004865,    0.004683,   0.0043224,   0.0041039,    0.004036,     0.00395,\n          0.0038169,   0.0036243,   0.0034353,   0.0032802,   0.0032272,   0.0029898,   0.0027521,    0.002495,   0.0023418,   0.0021988,   0.0020343,   0.0018916,   0.0017614,   0.0016259,   0.0016026,   0.0014228,   0.0013612,   0.0013019,   0.0012048,   0.0011577,   0.0011178,   0.0010643,  0.00099851,\n         0.00097176,  0.00093413,  0.00090178,  0.00088456,  0.00087071,  0.00085687,  0.00076324,  0.00067405,  0.00056792,  0.00049735,  0.00036649,  0.00036649,  0.00030708,  0.00026049,  0.00015527,  0.00010832,  8.0118e-05,  7.2069e-05,   6.402e-05,  5.5971e-05,  4.7922e-05,           0,           0,\n                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0]]), 'Confidence', 'Recall']]\nfitness: 0.36210131384804395\nkeys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\nmaps: array([    0.32438,     0.32438,     0.32438,     0.32438,     0.32438,     0.32438,     0.32438,     0.32438,     0.32438,     0.32438,     0.32438,     0.32438,     0.32438,     0.32438,     0.32438,     0.32438,     0.32438,     0.32438,     0.32438,     0.32438,     0.32438,     0.32438,     0.32438,     0.32438,\n           0.32438,     0.32438,     0.32438,     0.32438,     0.32438,     0.32438,     0.32438,     0.32438,     0.32438,     0.32438,     0.32438,     0.32438,     0.32438,     0.32438,     0.32438,     0.32438,     0.32438,     0.32438,     0.32438,     0.32438,     0.32438,     0.32438])\nnames: {0: 'Anorak', 1: 'Blazer', 2: 'Blouse', 3: 'Bomber', 4: 'Button-Down', 5: 'Cardigan', 6: 'Flannel', 7: 'Halter', 8: 'Henley', 9: 'Hoodie', 10: 'Jacket', 11: 'Jersey', 12: 'Parka', 13: 'Peacoat', 14: 'Poncho', 15: 'Sweater', 16: 'Tank', 17: 'Tee', 18: 'Top', 19: 'Turtleneck', 20: 'Capris', 21: 'Chinos', 22: 'Culottes', 23: 'Cutoffs', 24: 'Gauchos', 25: 'Jeans', 26: 'Jeggings', 27: 'Jodhpurs', 28: 'Joggers', 29: 'Leggings', 30: 'Sarong', 31: 'Shorts', 32: 'Skirt', 33: 'Sweatpants', 34: 'Sweatshorts', 35: 'Trunks', 36: 'Caftan', 37: 'Coat', 38: 'Coverup', 39: 'Dress', 40: 'Jumpsuit', 41: 'Kaftan', 42: 'Kimono', 43: 'Onesie', 44: 'Robe', 45: 'Romper'}\nplot: True\nresults_dict: {'metrics/precision(B)': 0.6960109549306018, 'metrics/recall(B)': 0.6770370973653134, 'metrics/mAP50(B)': 0.7015523535407865, 'metrics/mAP50-95(B)': 0.32438453165996145, 'fitness': 0.36210131384804395}\nsave_dir: WindowsPath('runs/detect/train')\nspeed: {'preprocess': 0.5464855193625776, 'inference': 25.09946355545138, 'loss': 0.0, 'postprocess': 0.8441132559723685}\ntask: 'detect'"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = YOLO('yolov8n.yaml')\n",
    "model.train(data='config.yaml', epochs=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T13:51:57.324693Z",
     "start_time": "2024-04-22T12:33:04.367761Z"
    }
   },
   "id": "4e41728303086083",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/1 C:\\Users\\elise\\Documents\\1 UTBM\\INFO 4 - DS\\TO52\\Abstract_Dotted_Blouse_img_00000070.jpg: 640x448 1 Blouse, 75.9ms\n",
      "Speed: 1.1ms preprocess, 75.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Results saved to \u001B[1mruns\\detect\\train122\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "results = model.predict(source='Abstract_Dotted_Blouse_img_00000070.jpg', save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-20T11:08:18.183519Z",
     "start_time": "2024-04-20T11:08:16.605516Z"
    }
   },
   "id": "58236e36d046a58c",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "torch.save(model.model, 'model_blouse.pt')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-20T11:20:57.266116Z",
     "start_time": "2024-04-20T11:20:57.245253Z"
    }
   },
   "id": "61213e4b67a76630",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not a mapping",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mblouse.pt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\engine\\model.py:321\u001B[0m, in \u001B[0;36mModel.save\u001B[1;34m(self, filename, use_dill)\u001B[0m\n\u001B[0;32m    313\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdatetime\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m datetime\n\u001B[0;32m    315\u001B[0m updates \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m    316\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdate\u001B[39m\u001B[38;5;124m\"\u001B[39m: datetime\u001B[38;5;241m.\u001B[39mnow()\u001B[38;5;241m.\u001B[39misoformat(),\n\u001B[0;32m    317\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mversion\u001B[39m\u001B[38;5;124m\"\u001B[39m: __version__,\n\u001B[0;32m    318\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlicense\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAGPL-3.0 License (https://ultralytics.com/license)\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    319\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdocs\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://docs.ultralytics.com\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    320\u001B[0m }\n\u001B[1;32m--> 321\u001B[0m torch\u001B[38;5;241m.\u001B[39msave({\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mckpt, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mupdates}, filename, use_dill\u001B[38;5;241m=\u001B[39muse_dill)\n",
      "\u001B[1;31mTypeError\u001B[0m: 'NoneType' object is not a mapping"
     ]
    }
   ],
   "source": [
    "model.save('blouse.pt')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-20T11:21:47.481328Z",
     "start_time": "2024-04-20T11:21:47.465613Z"
    }
   },
   "id": "dc9f3e2374e45c43",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.27 ðŸš€ Python-3.12.2 torch-2.2.1+cpu CPU (13th Gen Intel Core(TM) i9-13900HX)\n",
      "YOLOv8n summary (fused): 168 layers, 3014618 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\n",
      "\u001B[34m\u001B[1mPyTorch:\u001B[0m starting from 'runs\\detect\\train\\weights\\best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 50, 8400) (5.9 MB)\n",
      "\n",
      "\u001B[34m\u001B[1mTorchScript:\u001B[0m starting export with torch 2.2.1+cpu...\n",
      "\u001B[34m\u001B[1mTorchScript:\u001B[0m export success âœ… 1.0s, saved as 'runs\\detect\\train\\weights\\best.torchscript' (11.9 MB)\n",
      "\n",
      "Export complete (2.7s)\n",
      "Results saved to \u001B[1mC:\\Users\\elise\\Documents\\1 UTBM\\INFO 4 - DS\\TO52\\runs\\detect\\train\\weights\u001B[0m\n",
      "Predict:         yolo predict task=detect model=runs\\detect\\train\\weights\\best.torchscript imgsz=640  \n",
      "Validate:        yolo val task=detect model=runs\\detect\\train\\weights\\best.torchscript imgsz=640 data=config.yaml  \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": "'runs\\\\detect\\\\train\\\\weights\\\\best.torchscript'"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.export()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T13:55:39.551256Z",
     "start_time": "2024-04-22T13:55:36.860711Z"
    }
   },
   "id": "fb99f135ace65133",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e6becab5c8644f28"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
